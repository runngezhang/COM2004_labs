{
 "metadata": {
  "name": "",
  "signature": "sha256:d229ceb83ad85952be3ead26d403f83e4728474f48f785a10c4eb22582cf9630"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#COM2004/3004 Lab Week 7# \n",
      "\n",
      "#Feature Selection Challenge!#\n",
      "\n",
      "##Objective##\n",
      "\n",
      "You will be using data from a character recognition task where each character is represented by 900 pixels. \n",
      "\n",
      "The objective is to find a subset of 10 pixels that provide a good classification score. This will be done by using feature selection ideas discussed in the lectures. Some tools have been provided. The lab sheet will lead you through the initial steps to get you started.\n",
      "\n",
      "There will be a prize for the person (or team) who gets the highest score!\n",
      "\n",
      "***Note: Section 6 uses ideas that won't be covered until the next lecture.***\n",
      "\n",
      "##Background##\n",
      "\n",
      "In Part 2 of the assignment you will be using a character classifier that takes a 30x30 pixel image as input. Each image can therefore be represented as a 900-dimensional feature vector containing the pixel values. You should find that using these feature vectors and a nearest neighbour classifier you are able to achieve pretty accurate classification.\n",
      "\n",
      "One drawback of the nearest-neighbour classifier is that it has to store the complete training set in memory. This can be a problem if the feature vectors are large and memory is in short supply (e.g. in embedded systems). A further problem is that computing distances between large feature vectors can be computationally costly. It is therefore desirable to reduce the size of the feature vector. In this practical we are going to use feature selection to reduce the size of the feature vector from 900 to just 10 (i.e. 90 times less memory, 90 times less computation). It is important that we choose the 10 features carefully so that we do not loose too much classification performance.\n",
      "\n",
      "##1. Overview of key functions##\n",
      "\n",
      "The lab class will use three main functions. Code for these functions is included in the notebook.\n",
      "\n",
      "* classify -- a nearest neighbour classifier\n",
      "* divergence -- for measuring 1-D divergence\n",
      "* multidivergence \u2013 for measuring multi-dimensional divergence\n",
      "\n",
      "##2. Loading the lab data##\n",
      "\n",
      "The data for this week's lab class is stored in a MATLAB file called \"lab_data_wk7.mat\" that can be downloaded from the COM2004 MOLE site. \n",
      "\n",
      "**Download the lab_data_wk7.mat file.**\n",
      "\n",
      "The mat file can then be read using the Python's SciPy library function called 'loadmat' that can read MATLAB files. The matrices stored in the MATLAB file are returned as a Python dictionary where the names of the matrices are the keys and the matrix contents are the values. Each matrix can then be extract into a separate Python variable, i.e."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import io\n",
      "import numpy as np\n",
      "import urllib2\n",
      "\n",
      "mat_dict = io.loadmat(\"lab_data_wk7.mat\")\n",
      "\n",
      "print(mat_dict.keys())\n",
      "train_labels = mat_dict[\"train_labels\"]\n",
      "test_labels = mat_dict[\"test_labels\"]\n",
      "train_data = mat_dict[\"train_data\"].astype(np.float32)\n",
      "test_data = mat_dict[\"test_data\"].astype(np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The matrices train_data and test_data contain character data. Each row represents one feature vector which is comprised of the 900 pixel values for one character. There are 699 training characters and 200 for testing.\n",
      "\n",
      "To display a character as an image we first need to reshape the row of 900 pixel values into a 30 by 30 element matrix. We can do this using numpy's reshape function. We can then display the matrix as an image by using matplotlib's imshow function. \n",
      "\n",
      "Execute the cell below to display the first character in the training data. Then try editting and re-excuting the cell to display other characters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.cm as cm\n",
      "%matplotlib inline\n",
      "# display the 4th training data sample\n",
      "letter_image = np.reshape(train_data[3,:], (30,30), order='F')\n",
      "plt.imshow(letter_image, cmap=cm.Greys_r)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the cell below write code to display the first 16 training samples as a grid of 4x4 subplots."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SOLUTION"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The vectors called train_labels and test_labels store the character labels. The labels are represented using integers with 1=A, 2=B, 3=C, etc.\n",
      "\n",
      "The labels are stored in a matrix with 1 row and either 699 or 200 columns for the training and test data respectively. We can print the label for the Nth character in the training set by typing,\n",
      "\n",
      "    print(train_labels[0, N-1])\n",
      "    \n",
      "So for example the 10 character has label."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(train_labels[0,9])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Display the 10'th character image to check that it corresponds to the label."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SOLUTION"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3. Using the classify function##\n",
      "\n",
      "Below I have provided a function that performs nearest neighbour classification."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify(train, train_labels, test, test_labels, features=None):\n",
      "    \"\"\"Nearest neighbour classification.\n",
      "    \n",
      "    train - data matrix storing training data, one sample per row\n",
      "    train_label - a vector storing the training data labels\n",
      "    test - data matrix storing the test data\n",
      "    test_lables - a vector storing the test data labels for evaluation\n",
      "    features - a vector if indices that select the feature to use\n",
      "             if features=None then all features are used\n",
      "             \n",
      "    returns: (score, confusions) - a percentage correct and a \n",
      "                                  confusion matrix\n",
      "    \"\"\"\n",
      "    \n",
      "    # Use all feature is no feature parameter has been supplied\n",
      "    if features is None:\n",
      "        features=np.arange(0, train.shape[1])\n",
      "\n",
      "    # Select the desired features from the training and test data\n",
      "    train = train[:, features]\n",
      "    test = test[:, features]\n",
      "    \n",
      "    # Super compact implementation of nearest neighbour \n",
      "    x= np.dot(test, train.transpose())\n",
      "    modtest=np.sqrt(np.sum(test*test,axis=1))\n",
      "    modtrain=np.sqrt(np.sum(train*train,axis=1))\n",
      "    dist = x/np.outer(modtest, modtrain.transpose()); # cosine distance\n",
      "    nearest=np.argmax(dist, axis=1)\n",
      "    mdist=np.max(dist, axis=1)\n",
      "    label = train_labels[0,nearest]\n",
      "    score = (100.0 * sum(test_labels[0,:]==label))/label.shape[0]\n",
      "\n",
      "    # Construct a confusion matrix\n",
      "    nclasses = np.max(np.hstack((test_labels,train_labels)))\n",
      "    confusions=np.zeros((nclasses,nclasses))\n",
      "    for i in xrange(test_labels.shape[1]):\n",
      "        confusions[test_labels[0,i]-1,label[i]-1]+=1;\n",
      "\n",
      "    return score, confusions\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code is quite compact and is a little hard to understand. It has been written in a way that avoids using loops which ensures that it runs as quickly as possible. It is using a cosine distance rather than the more commonly-employed Euclidean distance. (The cosine distance is based on the angle between a pair of feature vectors rather than the distance between points.)\n",
      "\n",
      "Try out the code by executing the cell below,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score, confusions = classify(train_data, train_labels, test_data, test_labels)\n",
      "print(score)\n",
      "plt.matshow(confusions);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You should get a score of around 95%. This is higher would be achieved by the nearest neighbour classifier using a Euclidean distance. (Can you think why?) \n",
      "\n",
      "The classify function can take a 5th argument: a vector of integers representing the indexes of features to be selected. e.g. to test the classifier using just the 50th and 150th pixel in the image,\n",
      "\n",
      "    score, confusions = classify(train_data, train_labels, test_data, test_labels, [50 150])\n",
      "\n",
      "Try it out in the cell below. Performance now should be much lower!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SOLUTION"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**COMPETITON:** \n",
      "\n",
      "Your challenge is to find a vector of 10 feature indexes, such that,\n",
      "\n",
      "    classify(train_data, train_labels, test_data, test_labels, features)\n",
      "\n",
      "returns a result as close to 100% as possible. Email them to me and maybe win a prize!\n",
      "\n",
      "##4. Measuring Divergence##\n",
      "\n",
      "The cell below contains a function called 'divergence' that computes the divergence between a pair of classes for each feature in the feature vector. As input it takes two parameters: the first is a matrix of data for class 1, and the second is a matrix of data for class. The output will be a vector of divergence scores, i.e one score for each feature vector element.\n",
      "\n",
      "Study the code carefully,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def divergence(class1, class2):\n",
      "    \"\"\"compute a vector of 1-D divergences\n",
      "    \n",
      "    class1 - data matrix for class 1, each row is a sample\n",
      "    class2 - data matrix for class 2\n",
      "    \n",
      "    returns: d12 - a vector of 1-D divergence scores\n",
      "    \"\"\"\n",
      "\n",
      "    # Compute the mean and variance of each feature vector element\n",
      "    m1 = np.mean(class1, axis=0);\n",
      "    m2 = np.mean(class2, axis=0);\n",
      "    v1 = np.var(class1, axis=0);\n",
      "    v2 = np.var(class2, axis=0);\n",
      "\n",
      "    # Plug mean and variances into the formula for 1-D divergence.\n",
      "    # (Note that / and * are being used to compute multiple 1-D\n",
      "    #  divergences without the need for a loop)\n",
      "    d12 = 0.5*(v1/v2 + v2/v1 - 2) + 0.5 * ( m1-m2 ) * (m1-m2) * (1.0/v1+1.0/v2);\n",
      "\n",
      "    return d12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first couple of lines are computing the means and variances of each of the 900 pixels for each class. The last line is using the formula for 1-D divergence, but it works on a vector of means and variances, so it can compute 900 1-D divergences in a single go without needing a loop.\n",
      "\n",
      "The training data contains many classes (one class for each letter in the alaphabet). Divergence is measure between a pair of classes. So to use the divergence function we must first select feature vectors from train_data belonging to the pair of classes that we want to consider. This can be done using the train_label vector. For example, to select all the \u2018A\u2019s and \u2018B\u2019s run,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "adata = train_data[train_labels[0,:]==1,:];\n",
      "bdata = train_data[train_labels[0,:]==2,:];"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can measure the 900 separate 1-d divergences between the \u2018A\u2019 and \u2018B\u2019 class,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d12 = divergence(adata, bdata);\n",
      "print(d12.shape)\n",
      "print(d12)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see the divergences as an image execute the cell below,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "im = plt.imshow(np.reshape(d12,(30,30), order='F'), cmap=cm.Greys);\n",
      "plt.colorbar(im) # Add a colorbar to show the scale;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the above we have considered the divergence between the classes 'A' and 'B'. Use the cell below to experiment with other character pairs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SOLUTION"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##5. Feature Selection##\n",
      "\n",
      "Let\u2019s say that we now want to select features that will be good for separating \u2018A\u2019 s and \u2018B\u2019s. We need to choose features with high divergence. To do this we will sort the divergence values, d12, and look at the indexes of the highest values. We can do this with the numpy argsort function,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted_indexes = np.argsort(-d12)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Why are we sorting \u2013d12 and not d12?).\n",
      "\n",
      "The simplest feature selection algorithm would then just take the top 10 highest, i.e. the first 10 entries in the indexes vector. Type,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = sorted_indexes[0:9];\n",
      "score, confusions = classify(train_data, train_labels, test_data, test_labels, features);\n",
      "print(score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The overall performance might not be very good. Remember the features have only been selected on the basis of how well they separate A and B. The other 24 letters have not been considered.\n",
      "\n",
      "To examine the performance of the classifier in more detail we can look at a confusion matrix. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.imshow(confusions);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you look at the matrix you should see that not all the A\u2019s and B\u2019s have been correctly classified, but there are at least no cases of A\u2019s mistaken for B\u2019s or vice versa."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(confusions[0,1]) # number of A's mistaken as B's\n",
      "print(confusions[1,0]) # number of B's mistaken as A's"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##6. Improving the Feature Selection##\n",
      "\n",
      "*i/ Two classes versus many classes*\n",
      "\n",
      "To get a good overall classification score you need to find a set of features that have high divergence scores not just with respect to the A and B class, but for every pair of letters. Look again at Section 4 and consider how you might use a nested loop to call divergence repeatedly in such a way that you compute divergence between every pair of letters. \n",
      "\n",
      "How might all these pairwise scores be combined to compute some overall divergence score?\n",
      "\n",
      "Try constructing a multi-class divergence score and using it to select 10 features. Does the overall recognition score improve?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SOLUTION"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*ii/ Correlation between features*\n",
      "\n",
      "Remember, when selecting multiple features we can\u2019t estimate the overall divergence by simply summing the divergences of each individual feature. If the features are correlated the combined divergence is reduced. You can compute the correlation between each of the 900 features and each other 900 feature and store the results in a 900x900 matrix using,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corr = np.corrcoef(train_data, rowvar=0);\n",
      "plt.imshow(corr);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Can you explain the pattern of the corr image?)\n",
      "\n",
      "So when selecting your 10 features you want to find features for which the divergence is high but which have low correlation, e.g. compare the correlation between pixels 1 and 2 with the correlation between pixels 322 and 338, i.e. type,\n",
      "\n",
      "    print(corr[0,1])\n",
      "    print(corr[321,337])\n",
      "    \n",
      "In the lecture notes for the Week 7b lecture an algorithm is presented that iteratively selects features that have a *penalised* divergence where the penalty is based on degree of correlation to previously selected features. Try implementing this algorithm in the cell below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SOLUTION"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*iii/ Multivariate divergence*\n",
      "\n",
      "The function multidivergence below computes the multidimensional divergence while assuming that the data is normally distributed (next lecture). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def multidivergence(class1, class2, features):\n",
      "    \"\"\"compute divergence between class1 and class2\n",
      "    \n",
      "    class1 - data matrix for class 1, each row is a sample\n",
      "    class2 - data matrix for class 2\n",
      "    features - the subset of features to use\n",
      "    \n",
      "    returns: d12 - a scalar divergence score\n",
      "    \"\"\"\n",
      "\n",
      "    ndim=len(features);\n",
      "\n",
      "    # compute mean vectors\n",
      "    mu1 = np.mean(class1[:,features], axis=0);\n",
      "    mu2 = np.mean(class2[:,features], axis=0);\n",
      "\n",
      "    # compute distance between means\n",
      "    dmu = mu1 - mu2;\n",
      "\n",
      "    # compute covariance and inverse covariance matrices\n",
      "    cov1 = np.cov(class1[:,features], rowvar=0);\n",
      "    cov2 = np.cov(class2[:,features], rowvar=0);\n",
      " \n",
      "    icov1 = np.linalg.inv(cov1);\n",
      "    icov2 = np.linalg.inv(cov2);\n",
      "\n",
      "    # plug everything into the formula for multivariate gaussian divergence\n",
      "    d12 = 0.5 * np.trace(np.dot(icov1,cov2) + np.dot(icov2,cov1) - 2*np.eye(ndim)) + 0.5 * np.dot(np.dot(dmu,(icov1 +icov2)), dmu);\n",
      "\n",
      "    return d12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So you can test the divergence for a complete feature set as follows,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = [4, 240, 452, 789];   # Pick 4 features\n",
      "d12 = multidivergence(adata, bdata, features)\n",
      "print(d12)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using this function you could try implementing a sequential forward search like that discussed in the lectures. e.g. first of all find the 1-feature that produces the highest divergence\n",
      "\n",
      "    for i in xrange(nfeatures):\n",
      "        d[i] = multidivergence(adata, bdata, i);\n",
      "    index1 = np.argmax(d);\n",
      "\n",
      "Now try to find the best pair,\n",
      "\n",
      "    d=zeros(1, nfeatures);\n",
      "    for i in xrange(nfeatures):\n",
      "      if i is not index1:\n",
      "        d[i] = multidivergence(adata, bdata, [index1, i]);\n",
      "    index2 = np.argmax(d);\n",
      "    [index1, index2]\n",
      "\n",
      "Now repeat until you have found all 10.\n",
      "\n",
      "Again, if you want to get a good overall classification result you will need to use the ideas from 6,i) and sum the multidivergences over not just \u2018A\u2019 versus \u2018B\u2019 but over all letter pairs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SOLUTION"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##7. Competition##\n",
      "\n",
      "Find the 10 features that produce the best classification result. I don\u2019t mind whether you do this by guesswork or by using some fancy feature selection algorithm. For example,\n",
      "\n",
      "    features=[7, 74, 134, 162, 248, 278, 305, 741, 772, 821] \n",
      "    score, confusions = classify(train_data, train_labels, test_data, test_labels, features)\n",
      "    print(score)\n",
      "    \n",
      "Produces a score of 22.5%. Not very good. I believe it is possible to get the score up to at least 80%.\n",
      "\n",
      "Once you are happy that you have a good set of numbers, email them to me along with the score that they produce. Use COMPETITION as the subject title. Send the email before Sunday.\n",
      "\n",
      "I will announce the winner in the Monday lecture."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SOLUTION"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}