{
 "metadata": {
  "name": "",
  "signature": "sha256:c950f2ecedbd2118dab905337315d1154651407bb01dbcf0450d36d8e4fb3ce0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#COM2004/3004 Lab Week 7# \n",
      "\n",
      "#Feature Selection Challenge!#\n",
      "\n",
      "##Objective##\n",
      "\n",
      "Using data from a character recognition task, what is the best classification score that you can achieve using just 10 out of 900 available pixels?\n",
      "\n",
      "You will try to find the best performing set of 10 features by using the feature selection ideas discussed in the lectures. Some tools have been provided. The lab sheet will lead you through the initial steps to get you started.\n",
      "\n",
      "There will be a prize for the person (or team) who gets the highest score!\n",
      "\n",
      "##Background##\n",
      "\n",
      "In Part 2 of the assignment you will be using a typed-character classifier that takes a 30x30 pixel image as input. Each image can therefore be represented as a 900-dimensional feature vector containing the pixel values. You should find that using these feature vectors and a nearest neighbour classifier you are able to achieve pretty accurate classification.\n",
      "\n",
      "One drawback of the nearest-neighbour classifier is that it has to store the complete training set in memory. This can be a problem if the feature vectors are large and memory is in short supply (e.g. in embedded systems). Computing distances between large feature vectors can also take a long time. It is therefore desirable to reduce the size of the feature vector. In this practical we are going to use feature selection to reduce the size of the feature vector from 900 to just 10 (i.e. 90 times less memory, 90 times less computation). It is important that we choose the 10 features carefully so that we do not loose too much classification performance.\n",
      "\n",
      "##1. Overview of key functions##\n",
      "\n",
      "The lab class will use three main functions. Code for these functions is included in the notebook.\n",
      "\n",
      "* classify -- a nearest neighbour classifier\n",
      "* divergence -- for measuring 1-D divergence\n",
      "* multidivergence \u2013 for measuring multi-dimensional divergence\n",
      "\n",
      "##2. Loading the lab data##\n",
      "\n",
      "The data for this week's lab class is stored in a MATLAB file called \"data/lab_data_wk7.mat\". The Python's SciPy library provides a function called loadmat that can read MATLAB files. The matrices stored in the MATLAB file are returned as a Python dictionary where the names of the matrices are the keys and the matrix contents are the values. Each matrix can then be extract into a separate Python variable, i.e."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import io\n",
      "import numpy as np\n",
      "\n",
      "mat_dict = io.loadmat(\"data/lab_data_wk7.mat\")\n",
      "print(mat_dict.keys())\n",
      "train_labels = mat_dict[\"train_labels\"]\n",
      "test_labels = mat_dict[\"test_labels\"]\n",
      "train_data = mat_dict[\"train_data\"].astype(np.float32)\n",
      "test_data = mat_dict[\"test_data\"].astype(np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The matrices train_data and test_data contain a portion of the data that is being used in the assignment, but here it is being stored in matrix form. Each row represents one feature vector and contains the 900 pixel values for one character. There are 699 training characters and 200 for testing.\n",
      "\n",
      "To display a character as an image we first need to reshape the row of 900 pixel values into a 30 by 30 element matrix. We can do this using numpy's reshape function. We can then display the matrix as an image by using matplotlib's imshow function. \n",
      "\n",
      "Execute the cell below to display the first character in the training data. Then try editting and re-excuting the cell to display other characters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.cm as cm\n",
      "%matplotlib inline\n",
      "letter_image = np.reshape(train_data[3,:], (30,30), order='F')\n",
      "plt.imshow(letter_image, cmap=cm.Greys_r)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The vector train_labels and test_labels store the character labels as integers using a code where 1=A, 2=B, 3=C.\n",
      "\n",
      "The labels are stored in a matrix with 1 row and either 699 or 200 columns for the training and test data respectively. We can print the label for the Nth character in the training set by typing,\n",
      "\n",
      "    print(train_labels[0, N-1])\n",
      "    \n",
      "So for example the 10 character has label."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(train_labels[0,9])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Display the 10'th character image to check that this label is correct.\n",
      "\n",
      "##3. Using the classify.m code##\n",
      "\n",
      "Below I have provided a function that performs nearest neighbour classification."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify(train, train_labels, test, test_labels, features=None):\n",
      "\n",
      "# Nearest neighbour classification.\n",
      "\n",
      "    # Use all feature is no feature parameter has been supplied\n",
      "    if features is None:\n",
      "        features=np.arange(0, train.shape[1])\n",
      "\n",
      "    # Select the desired features from the training and test data\n",
      "    train = train[:, features]\n",
      "    test = test[:, features]\n",
      "    \n",
      "    # Super compact implementation of nearest neighbour \n",
      "    x= np.dot(test, train.transpose())\n",
      "    modtest=np.sqrt(np.sum(test*test,axis=1))\n",
      "    modtrain=np.sqrt(np.sum(train*train,axis=1))\n",
      "    dist = x/np.outer(modtest, modtrain.transpose()); # cosine distance\n",
      "    nearest=np.argmax(dist, axis=1)\n",
      "    mdist=np.max(dist, axis=1)\n",
      "    label = train_labels[0,nearest]\n",
      "    score = (100.0 * sum(test_labels[0,:]==label))/label.shape[0]\n",
      "\n",
      "    # Construct a confusion matrix\n",
      "    nclasses = np.max(np.hstack((test_labels,train_labels)))\n",
      "    confusions=np.zeros((nclasses,nclasses))\n",
      "    for i in xrange(test_labels.shape[1]):\n",
      "        confusions[test_labels[0,i]-1,label[i]-1]+=1;\n",
      "\n",
      "    return score, confusions\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code is quite compact and is a little hard to understand. It has been written in a way that avoids using loops which ensures that it runs as quickly as possible. It is using a cosine distance rather than the more commonly-employed Euclidean distance. (The cosine distance is based on the angle between a pair of feature vectors rather than the distance between points.)\n",
      "\n",
      "Try out the code by typing,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score, confusions = classify(train_data, train_labels, test_data, test_labels)\n",
      "print(score)\n",
      "plt.imshow(confusions);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You should get a score of around 95% ... rather higher than achieved with a Euclidean distance. (Why?) \n",
      "\n",
      "The function classify.m can take a 5th argument: a vector of integers representing the indexes of features to be selected. e.g. to test the classifier using just the 50th and 150th pixel in the image,\n",
      "\n",
      "    score, confusions = classify(train_data, train_labels, test_data, test_labels, [50 150])\n",
      "\n",
      "Try it out in the cell below. Performance now should be much lower!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score, confusions = classify(train_data, train_labels, test_data, test_labels, [50, 150, 250, 350])\n",
      "print(score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**COMPETITON:** \n",
      "\n",
      "Your challenge is to find a vector of 10 numbers, call it, features, such that,\n",
      "\n",
      "    classify(train_data, train_labels, test_data, test_labels, features)\n",
      "\n",
      "returns a result as close to 100% as possible. Email them to me and maybe win a prize!\n",
      "\n",
      "##4. Measuring Divergence##\n",
      "\n",
      "The function divergence in the cell below computes the 1-D divergence between a pair of classes, i.e. each feature is considered separately and is modeled by a Gaussian distribution. As input it takes two parameters: the first is a matrix of data for class 1, and the second is a matrix of data for class. \n",
      "\n",
      "Study the code carefully,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def divergence(class1, class2):\n",
      "# compute a vector of 1-D divergences \n",
      "# feature vectors for the two classes are stored as the rows of class1 and class2\n",
      "\n",
      "    # Compute the mean and variance of each vector element\n",
      "    m1 = np.mean(class1, axis=0);\n",
      "    m2 = np.mean(class2, axis=0);\n",
      "    v1 = np.var(class1, axis=0);\n",
      "    v2 = np.var(class2, axis=0);\n",
      "\n",
      "    # Plug mean and variances into the formula for 1-D divergence.\n",
      "    # (Note that / and * are being used to compute multiple 1-D\n",
      "    #  divergences without the need for a loop)\n",
      "    d12 = 0.5*(v1/v2 + v2/v1 - 2) + 0.5 * ( m1-m2 ) * (m1-m2) * (1.0/v1+1.0/v2);\n",
      "\n",
      "    return d12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first couple of lines are computing the means and variances of each of the 900 pixels for each class. The last line is using the formula for 1-D divergence, but it works on a vector of means and variances, so it can compute 900 1-D divergences in a single go without needing a loop.\n",
      "\n",
      "To use divergence function we must first select feature vectors from train_data belonging the pair of classes that we want to consider. This can be done using the train_label vector. For example, to select all the \u2018A\u2019s and \u2018B\u2019s run,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "adata = train_data[train_labels[0,:]==1,:];\n",
      "bdata = train_data[train_labels[0,:]==2,:];"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can measure the 900 separate 1-d divergences between the \u2018A\u2019 and \u2018B\u2019 class,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d12 = divergence(adata, bdata);\n",
      "print(d12.shape)\n",
      "print(d12[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see the divergences as an image type,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "im = plt.imshow(np.reshape(d12,(30,30), order='F'), cmap=cm.Greys);\n",
      "plt.colorbar(im) # Add a colorbar to show the scale;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##5. Feature Selection##\n",
      "\n",
      "Let\u2019s say that we now want to select features that will be good for separating \u2018A\u2019 s and \u2018B\u2019s. We need to choose features with high divergence. To do this we will sort the divergence values, d12, and look at the indexes of the highest values. We can do this with the numpy argsort function,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted_indexes = np.argsort(-d12);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Why are we sorting \u2013d12 and not d12?).\n",
      "The simplest feature selection algorithm would then just take the top 10 highest, i.e. the first 10 entries in the indexes vector. Type,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = sorted_indexes[0:9];\n",
      "score, confusions = classify(train_data, train_labels, test_data, test_labels, features);\n",
      "print(score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The overall performance might not be very good. Remember the features have only been selected on the basis of how well they separate A and B. The other 24 letters have not been considered.\n",
      "\n",
      "To examine the performance of the classifier in more detail we can look at a confusion matrix. The function classify.m will produce a confusion matrix if it is called like this"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.imshow(confusions);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you look at the matrix you should see that not all the A\u2019s and B\u2019s have been correctly classified, but there are at least no cases of A\u2019s mistaken for B\u2019s or vice versa."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(confusions[0,1]) # number of A's mistaken as B's\n",
      "print(confusions[1,0]) # number of B's mistaken as A's"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##6. Improving the Feature Selection##\n",
      "\n",
      "*i/ Two classes versus many classes*\n",
      "\n",
      "To get a good overall classification score you need to find a set of features that have high divergence scores not just with respect to the A and B class, but for every pair of letters. Look again at Section 4 and consider how you might use a nested loop to call divergence repeatedly in such a way that you compute divergence between every pair of letters. \n",
      "\n",
      "How might all these pairwise scores be combined to compute some overall divergence score?\n",
      "\n",
      "Try constructing a multi-class divergence score and using it to select 10 features. Does the overall recognition score improve?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*ii/ Correlation between features*\n",
      "\n",
      "Remember, when selecting multiple features we can\u2019t estimate the overall divergence by simply summing the divergences of each individual feature. If the features are correlated the combined divergence is reduced. You can compute the correlation between each of the 900 features and each other 900 feature and store the results in a 900x900 matrix using,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corr = np.corrcoef(train_data, rowvar=0);\n",
      "plt.imshow(corr);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Can you explain the pattern of the corr image?)\n",
      "\n",
      "So when selecting your 10 features you want to find features for which the divergence is high but which have low correlation, e.g. compare the correlation between pixels 1 and 2 with the correlation between pixels 322 and 338, i.e. type,\n",
      "\n",
      "    print(corr[0,1])\n",
      "    print(corr[321,337])\n",
      "    \n",
      "In the lecture notes an algorithm was presented the iteratively selects features that have a *penalised* divergence where the penalty is based on degree of correlation to previously selected features. Try implementing this algorithm in the cell below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*iii/ Multivariate divergence*\n",
      "\n",
      "The function multidivergence below computes the multidimensional divergence while assuming that the data is normally distributed (next lecture). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def multidivergence(class1, class2, features):\n",
      "\n",
      "    ndim=len(features);\n",
      "\n",
      "    # compute mean vectors\n",
      "    mu1 = np.mean(class1[:,features], axis=0);\n",
      "    mu2 = np.mean(class2[:,features], axis=0);\n",
      "\n",
      "    # compute distance between means\n",
      "    dmu = mu1 - mu2;\n",
      "\n",
      "    # compute covariance and inverse covariance matrices\n",
      "    cov1 = np.cov(class1[:,features], rowvar=0);\n",
      "    cov2 = np.cov(class2[:,features], rowvar=0);\n",
      " \n",
      "    icov1 = np.linalg.inv(cov1);\n",
      "    icov2 = np.linalg.inv(cov2);\n",
      "\n",
      "    # plug everything into the formula for multivariate gaussian divergence\n",
      "    d12 = 0.5 * np.trace(np.dot(icov1,cov2) + np.dot(icov2,cov1) - 2*np.eye(ndim)) + 0.5 * np.dot(np.dot(dmu,(icov1 +icov2)), dmu);\n",
      "\n",
      "    return d12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So you can test the divergence for a complete feature set as follows,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = [4, 240, 452, 789];   # Pick 4 features\n",
      "d12 = multidivergence(adata, bdata, features)\n",
      "print(d12)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using this function you could try implementing a sequential forward search like that discussed in the lectures. e.g. first of all find the 1-feature that produces the highest divergence\n",
      "\n",
      "    for i in xrange(nfeatures):\n",
      "        d[i] = multidivergence(adata, bdata, i);\n",
      "    index1 = np.argmax(d);\n",
      "\n",
      "Now try to find the best pair,\n",
      "\n",
      "    d=zeros(1, nfeatures);\n",
      "    for i in xrange(nfeatures):\n",
      "      if i is not index1:\n",
      "        d[i] = multidivergence(adata, bdata, [index1, i]);\n",
      "    index2 = np.argmax(d);\n",
      "    [index1, index2]\n",
      "\n",
      "Now repeat until you have found all 10.\n",
      "\n",
      "Again, if you want to get a good overall classification result you will need to use the ideas from 6,i) and sum the multidivergences over not just \u2018A\u2019 versus \u2018B\u2019 but over all letter pairs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add you code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##7. Competition##\n",
      "\n",
      "Find the 10 features that produce the best classification result. I don\u2019t mind whether you do this by guesswork or by using some fancy feature selection algorithm. For example,\n",
      "\n",
      "    features=[7, 74, 134, 162, 248, 278, 305, 741, 772, 821] \n",
      "    score, confusions = classify(train_data, train_labels, test_data, test_labels, features)\n",
      "    print(score)\n",
      "    \n",
      "Produces a score of 22.5%. Not very good. I believe it is possible to get the score up to at least 80%.\n",
      "\n",
      "Once you are happy that you have a good set of numbers, email them to me along with the score that they produce. Use COMPETITION as the subject title. Send the email before Sunday.\n",
      "\n",
      "I will announce the winner in the Monday lecture."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    }
   ],
   "metadata": {}
  }
 ]
}