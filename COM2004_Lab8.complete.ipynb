{
 "metadata": {
  "name": "",
  "signature": "sha256:5560a1e9d9c6b128a44286892e54d33499ac4bc5bb3de1140ec38e020930428b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#COM2004/3004 Lab Week 8#\n",
      "\n",
      "#Principal Component Analysis#\n",
      "\n",
      "##Objective##\n",
      "\n",
      "In the lab you will use Principal Components Analysis to produce a compact representation of the letter image data. You will demonstrate that this compact representation provides a basis for robust letter classification.\n",
      "\n",
      "##Background##\n",
      "\n",
      "In last week\u2019s lab you performed dimensionality reduction using simple feature selection. Using a combination of brute force and ingenuity you found it was possible to select individual pixel features that allowed robust classification with feature vectors with only 10 elements. This week you will perform a similar degree of dimensionality reduction but this time using a technique called Principal Components Analysis (PCA) which will construct features that are a linear combination of the original pixel values. It will produce good performance without the need for a complex feature selection algorithm.\n",
      "\n",
      "##1. Overview of key functions##\n",
      "\n",
      "The lab class will use two main functions that we have seen before. Code for these functions is included in the notebook.\n",
      "\n",
      "* divergence.m - for measuring 1-D divergence\n",
      "* classify.m - a nearest neighbour classifier\n",
      "\n",
      "##2. Loading the lab data##\n",
      "\n",
      "The data is stored in a MATLAB file called lab_data_wk8. We will load this in using the loadmat function that we saw last week. The data is similar to that used last week except that there are now two separate test sets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import io\n",
      "import numpy as np\n",
      "\n",
      "mat_dict = io.loadmat(\"data/lab_data_wk8.mat\")\n",
      "print(mat_dict.keys())\n",
      "train_labels = mat_dict[\"train_labels\"]\n",
      "test_labels = mat_dict[\"test_labels\"]\n",
      "test2_labels = mat_dict[\"test2_labels\"]\n",
      "train_data = mat_dict[\"train_data\"].astype(np.float32)\n",
      "test_data = mat_dict[\"test_data\"].astype(np.float32)\n",
      "test2_data = mat_dict[\"test2_data\"].astype(np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Similarly to last week, the matrices train_dat, test_data and test2_data contain 699, 200 and 200 feature vectors respectively stored in a matrix form. Each row represents one feature vector and contains the 900 pixel values for one character.\n",
      "\n",
      "Remember, you can view the ith character in the training set by typing, \n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.cm as cm\n",
      "%matplotlib inline\n",
      "letter_image = np.reshape(train_data[0,:], (30,30), order='F');\n",
      "plt.imshow(letter_image, cmap=cm.Greys_r);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The vector train_labels and test_labels store the character labels as integers using a code where 1=A, 2=B, 3=C. Try displaying some other characters.\n",
      "\n",
      "##3. Principal Component Analysis \u2013 a primer##\n",
      "\n",
      "*We will be covering PCA in detail in the lecture session on Friday. This section is intended to give you a brief overview that will help you understand what is happening in today\u2019s lab.*\n",
      "\n",
      "As you have seen, the letter images in your training and test sets are represented by 900- dimensional feature vectors. Hence, any particular letter image can be pictured as a single point in a 900-d space. The complete training set is therefore a collection of 699 points in in 900-d space, i.e. the training data forms a small cluster in 900-d space. Now, this cluster will be more spread about its centre in some directions than in others. The PCA technique simply identifies a set of orthogonal directions in which the amount of spread (i.e. variance) is the greatest. i.e. consider first finding the one direction in which the spread is greatest; then find a direction in which the spread is greatest but which is also at right-angles to the first direction found; then find a direction in which the spread is greatest but that is at right angle to the first two directions; and so on. (Remember than in 900-d space there can be 900 mutually perpendicular directions.) These directions \u2013 ordered in terms of spread -- will be known as principal components.\n",
      "\n",
      "*How does finding the principal components allow us to perform dimensionality reduction?* Well, once the first few principal components have been found we can measure the distance of a point from the cluster center (i.e. from the mean) along these few directions, e.g. perhaps 10 directions. Now we can represent the 900-d point by these 10 measurements. Measuring the point positions along these directions is known as \u2018projecting onto the principal component axes\u2019. Crucially, because the directions are ordered by the size of the spread in that direction, the position along the first, say 10, principal components axes will approximate the position of the point in 900-d space, i.e. the other 890 orthogonal direction are less important. This projection step is simple to compute, it is actually just a linear transform of the 900-d vector, x, i,e, y = A x.\n",
      "\n",
      "There is another way of thinking about the principal components. Each axis direction can be represented as a 900-d vector. This vector itself could be visualized as a 30 by 30 image, i.e. each principal component is now being conceptualized as an image (a 'basis image\u2019) rather than as an axis. Recall that when we projected onto the principal component axes we were taking a 900-d feature vector and describing its position relative to the centre of the cluster as a sum of movements along the principal component axes directions. In the alternative view, this is equivalent to describing the original image as being formed by an average image plus a weighted sum of the basis images, i.e. the position of the point along each principal component axes is telling us by how much to weight the corresponding basis image in the sum. i.e. we can approximate every image in the training and test set as the weighted sum of a small number of basis images.\n",
      "\n",
      "All of the above would not be very interesting if it was not possible to actually compute the principal components. Fortunately it turns out \u2013 as will be explained in the lecture \u2013 that the principal components are the eigenvectors of the data\u2019s covariance matrix. This might sound complicated, but it is just a few lines of MATLAB.\n",
      "\n",
      "*Do not worry if you did not completely follow all of the above. Reread it at the end of the lab class and come along to the lecture on Friday.*\n",
      "\n",
      "##4. Computing the Principal Components##\n",
      "\n",
      "As we will see in the lecture on Friday, the principal components are simply the eigenvectors of the covariance matrix. They can be computed using the training data with just two lines of MATLAB code. e.g. to compute the first 40 principal components use the following,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "covx = np.cov(train_data, rowvar=0)\n",
      "N = covx.shape[0]\n",
      "import scipy\n",
      "w, v = scipy.linalg.eigh(covx, eigvals=(N-40, N-1))\n",
      "v = np.fliplr(v)\n",
      "v.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function eigs will return the eigenvectors (i.e. principal component axes) as column vectors in the matrix V. Check the sizes of covx and V using \u2018size(covx)\u2019 and \u2018size(V)\u2019. Make sure that you understand why these matrices have the sizes that they have.\n",
      "\n",
      "You can view the principal components as basis images ('eigenletters' ?) by reshaping them into a 30 by 30 matrix. Look at the first 4,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.subplot(2,2,1)\n",
      "plt.imshow(np.reshape(v[:, 0], (30, 30), order='F'), cmap=cm.Greys_r)\n",
      "plt.subplot(2,2,2)\n",
      "plt.imshow(np.reshape(v[:, 1], (30, 30), order='F'), cmap=cm.Greys_r)\n",
      "plt.subplot(2,2,3)\n",
      "plt.imshow(np.reshape(v[:, 2], (30, 30), order='F'), cmap=cm.Greys_r)\n",
      "plt.subplot(2,2,4)\n",
      "plt.imshow(np.reshape(v[:, 3], (30, 30), order='F'), cmap=cm.Greys_r)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "PCA will represent the original letter data as a weighted sum of these eigenletter images. Loosely speaking, they are like the common components from which the set of letter images are 'built\u2019.\n",
      "\n",
      "##4. Projecting the data onto the principal component axes##\n",
      "\n",
      "We will now perform the actual dimensionality reduction by projecting the 900 dimensional images onto the first 40 principal components, i.e. the \u2018linear transform, y=Vx\u2019 (actually, because, oppositely to the lecture notes, our images are stored as row vectors and our principal components as column vectors, the equation looks like y = x V; don\u2019t let this confuse you). Also, it is necessary to 'centre\u2019 the data before transforming it by subtracting the mean letter vector. So we have,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pcatrain_data = np.dot((train_data - np.mean(train_data)), v);\n",
      "pcatest_data = np.dot((test_data - np.mean(train_data)), v);\n",
      "pcatest2_data = np.dot((test2_data - np.mean(train_data)), v);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The \u2018np.tile\u2019 command copies the mean vector so that it can be subtracted from every training or test vector in the dataset in one go, without needing a loop.\n",
      "\n",
      "Because we have only used the first 40 principal components some information has been lost. But because the principal component are ordered by the amount of variance that they capture, the amount of information lost will be minimized. (Note, the word \u2018information\u2019 is being used in a rather wooly way here, but the above statement is true in a more technical sense as long as certain assumptions can be made about the distribution of the data. But these details needn\u2019t overly concern us.)\n",
      "\n",
      "To see how closely the original image can be reconstructed we can project back from the 40-d space to the original 900 dimension. After projecting back we should remember to undo the centering by adding back the mean vector. This can be done simply by"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reconstructed = np.dot(pcatrain_data, v.transpose()) + np.mean(train_data);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now display one of the reconstructed characters and compare it to the original,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.reshape(train_data[0,:], (30,30), order='F'), cmap=cm.Greys_r);\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(np.reshape(reconstructed[0,:], (30,30), order='F'), cmap=cm.Greys_r);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You should see that the reconstructed letter looks very similar to the original. Notice how, because only 40 PCA components have been used rather than all 900, the image is somewhat 'smoothed\u2019. This is a good thing \u2013 the largely irrelevant noise component has been effectively removed.\n",
      "\n",
      "To project just the first image from the training data set into N-dimensional PCA space and back again we can use,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 6;   \n",
      "mean_train = np.mean(train_data)\n",
      "reconstructed = np.dot(np.dot(train_data[0,:]-mean_train, v[:,0:N-1]), v[:,0:N-1].transpose()) + mean_train;\n",
      "plt.subplot(1,2,1)\n",
      "plt.imshow(np.reshape(train_data[0,:], (30,30), order='F'), cmap=cm.Greys_r);\n",
      "plt.subplot(1,2,2)\n",
      "plt.imshow(np.reshape(reconstructed, (30,30), order='F'), cmap=cm.Greys_r);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Experiment with different values of N. The figure below shows an example using the letter 'O' in train_data(1,:) with N equal to 1, 2, 3, 4, 5 and 6. Notice how the 'O'ness of the \u2018O\u2019 is captured by just the first 4 principal components! Experiment with different letters. For example, how many components are needed to capture the 'A'ness of an 'A'?\n",
      "\n",
      "![PCA-ed Ohs](graphics/Ohs.png)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##5. Performing the classification##\n",
      "\n",
      "We can now test our nearest neightbour classifier on the PCA\u2019ed data. Here is the code for the classifier again,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify(train, train_labels, test, test_labels, features=None):\n",
      "\n",
      "# Nearest neighbour classification.\n",
      "\n",
      "    # Use all feature is no feature parameter has been supplied\n",
      "    if features is None:\n",
      "        features=np.arange(0, train.shape[1])\n",
      "\n",
      "    # Select the desired features from the training and test data\n",
      "    train = train[:, features]\n",
      "    test = test[:, features]\n",
      "    \n",
      "    # Super compact implementation of nearest neighbour \n",
      "    x= np.dot(test, train.transpose())\n",
      "    modtest=np.sqrt(np.sum(test*test,axis=1))\n",
      "    modtrain=np.sqrt(np.sum(train*train,axis=1))\n",
      "    dist = x/np.outer(modtest, modtrain.transpose()); # cosine distance\n",
      "    nearest=np.argmax(dist, axis=1)\n",
      "    mdist=np.max(dist, axis=1)\n",
      "    label = train_labels[0,nearest]\n",
      "    score = (100.0 * sum(test_labels[0,:]==label))/label.shape[0]\n",
      "\n",
      "    # Construct a confusion matrix\n",
      "    nclasses = np.max(np.hstack((test_labels,train_labels)))\n",
      "    confusions=np.zeros((nclasses,nclasses))\n",
      "    for i in xrange(test_labels.shape[1]):\n",
      "        confusions[test_labels[0,i]-1,label[i]-1]+=1;\n",
      "\n",
      "    return score, confusions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will first test it using 40 PCA features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score, cm = classify(pcatrain_data, train_labels, pcatest_data, test_labels, xrange(40));\n",
      "print(score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now try with just 10,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score, cm = classify(pcatrain_data, train_labels, pcatest_data, test_labels, xrange(10));\n",
      "print(score)\n",
      "# Scoring much lower than MATLAB version!! JPB"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Results may seem a little disappointing, probably about 88% and 83%. This is not so high compared with the figures in the 90\u2019s that could be achieved with feature selection. But now try classifying again but this time using PCA components 2 to 11,\n",
      "\n",
      "    classify(pcatrain_data, train_labels, pcatest_data, test_labels,2:11);\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score, cm = classify(pcatrain_data, train_labels, pcatest_data, test_labels, xrange(1,11));\n",
      "print(score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Classification performance when using features 2 to 11 should be considerably better than when using features 1 to 10 (try and think why.). Performance should now be comparable to the best results that were achieved in Week 7. But note, these results have been achieved with considerably less 'fuss\u2019. e.g. there was no trial and error or brute force search. PCA just worked. It will work equally well for many similar problems and for larger and more difficult data sets.\n",
      "\n",
      "##6. Feature Selection##\n",
      "\n",
      "Typically, with PCA, we would take the first N PCA components as our features. However, let us try combining PCA and the feature selection ideas from last week. Starting with the 40 principal components that you have computed can you find the 10 that give best performance?\n",
      "\n",
      "You could try using trial and error or you could try reusing the divergence code from last week: Compute the 1-D divergences for the PCA features. Remember you will need some way of summing divergences over pairs of classes. Rank the PCA features according to their 1-D divergence and simply pick the best 10, i.e. assume that the 10-D divergence can be estimated as the sum of the 10 1-D divergences. (It is more valid to do this when using PCA features than when using raw pixel feaatures. Why?)\n",
      "\n",
      "Test your feature selection on the test2 dataset. Can you score higher than the highest score that we achieved by selecting individual pixels, i.e. 94%? I haven\u2019t tried this myself, so I will be genuinely interested to see if there is a 10-d feature set that can beat the set [2,3,4,5,6,7,8,9,10,11] that we used in Section 5."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##7. Robustness to noise (if you have time)##\n",
      "\n",
      "Try adding Gaussian noise to the 900-dimensional test data, (i.e. random numbers generated using randn.) Now, transform the noisy data into the PCA domain and rerun the classifier. Compare the noise robustness of the original 900-d feature vectors (assignment stage 1) and the 10-d PCA based features (this week). Which of these feature vectors can tolerate the greatest amount of added noise in the test data? Why do you think this is? Again, I haven\u2019t tried this myself and will be interested to hear what you find."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    }
   ],
   "metadata": {}
  }
 ]
}