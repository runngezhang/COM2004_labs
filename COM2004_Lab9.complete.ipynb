{
 "metadata": {
  "name": "",
  "signature": "sha256:38986ef1937e6089fd610f9884631b8059669be1827b0a82e3ce7f9f9d0cfaf6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#COM2004/3004 Lab Week 9#\n",
      "\n",
      "#Agglomerative Clustering Challenge#\n",
      "\n",
      "##Objective##\n",
      "\n",
      "In this lab class you will gain some practical experience with hierarchical agglomerative clustering. You will start by playing with some 2D data and seeing how different distance measures can produce different clustering results. You will then apply clustering techniques to the word search letter data. You will be challenged to find a 10- dimensional representation of the letters that allows the data to be reliably clustered. There will be a prize for the best result.\n",
      "\n",
      "##Background##\n",
      "\n",
      "In the lectures we have been discussing clustering. We have seen that clustering lies at the heart of many data driven applications. In particular, it can be used to solve problems in situations where we do not have access to labeled training data. For example, the word search problem (assignment stage 2) could be solved entirely without labeled data: We could take our training letter images and simply cluster them into 26 groups. We would hope that each letter would fall into a separate cluster. Each cluster could then be assigned an arbitrary label (e.g. integers 1 to 26). We would then take an image of a word search grid and of the words to be found. We would classify the letters in the grid and the letters in the search words using the arbitrary labels (1 to 26). Finally we would search for the words by matching the label sequences of the search words to the labels in the grid.\n",
      "\n",
      "For the above \u2018unsupervised\u2019 word search solving approach to work, it would be necessary for the initial clustering step to be accurate, i.e. each cluster should only contain examples of one letter. In today\u2019s practical we are going to see if an accurate clustering can be achieved using a simple *agglomerative hierarchical* clustering approach. (The algorithm will be explained in detail in the lecture on Friday).\n",
      "\n",
      "##1. Overview of key functions##\n",
      "\n",
      "This week's lab will make use of the following functions that are provided later in the notebook, \n",
      "\n",
      "    makeClusters \u2013 for making random clustered 2D data.\n",
      "    cluster \u2013 some flexible agglomerative clustering code. \n",
      "    doPCA \u2013 code for applying PCA transform to data.\n",
      "\n",
      "##2. Loading the lab data##\n",
      "\n",
      "Load the data from the lab_data_wk9 MATLAB file by executing the following cell "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import io\n",
      "import numpy as np\n",
      "\n",
      "mat_dict = io.loadmat(\"data/lab_data_wk9.mat\")\n",
      "print(mat_dict.keys())\n",
      "test_labels = mat_dict[\"test_labels\"]\n",
      "test_data = mat_dict[\"test_data\"].astype(np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following familiar variables should appear in your MATLAB workspace,\n",
      "     \n",
      "     test_data,  test_labels\n",
      "\n",
      "As in previous weeks, the matrix test_data contains a set of feature vectors arranged as rows. Each feature vector contains the 900 pixel values that when arranged as a 30 by 30 matrix form the image of a single character. The character labels have been provided and are stored in the variable test_labels. The labels will not be needed by the clustering algorithm (remember, clustering is unsupervised), however, it is useful to have access to the labels so that we can eventually evaluate how well the clustering algorithm has performed.\n",
      "\n",
      "Remember, you can view the $i$th character in the data set by typing, "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.cm as cm\n",
      "%matplotlib inline\n",
      "i = 10\n",
      "plt.imshow(np.reshape(test_data[i-1, :], (30,30), order='F'), cmap=cm.Greys_r) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3. Clustering 2D data##\n",
      "\n",
      "We will start by testing the clustering code on some 2D data. We can plot 2D data using scatter plots so it is easy to see exactly what is going on. A simple function called makeClusters has been provided below that can generate a set of 2D points that fall into clusters. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def makeClusters(nclusters, clustersize, spread=None):\n",
      "    if spread is None:\n",
      "        spread = 20.0\n",
      "\n",
      "    x = None\n",
      "    for i in xrange(nclusters):\n",
      "        pmean = np.random.standard_normal((2, )) * 100;\n",
      "        pcov = np.random.standard_normal((2, 2)) * spread\n",
      "        pcov[0, 1] = pcov[1, 0] = 0\n",
      "        y = np.random.multivariate_normal(pmean, pcov, clustersize)\n",
      "        x = y if x is None else np.vstack((x, y))\n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The makeClusters program takes two parameters,\n",
      "\n",
      "    x = makeClusters(number_of_clusters, points_per_cluster)\n",
      "\n",
      "It will generate 2D points stored in a matrix x with a separate row for each point. For example, to\n",
      "make 10 clusters with 10 points in each cluster type\n",
      "     \n",
      "    x = makeClusters(10,10)\n",
      "\n",
      "We can now display the generated points using a scatter plot as follows,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = makeClusters(10, 10, 20)\n",
      "plt.scatter(x[:,0], x[:,1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each time you run makeClusters you will get a different result, so if the clusters are not nicely\n",
      "spread out run it a few more times until they are.\n",
      "\n",
      "We will now use the clustering program to cluster the data in x. Type (or cut and paste) the\n",
      "following command *very* carefully,\n",
      "\n",
      "    cluster(x,10,'lambda x,y: dm_cc_min(x,y,dm_pp_euclid)','display_points');\n",
      "\n",
      "You should see an animation of the points clustering with colours being used to indicate which points belong to which clusters. Does the final clustering seem reasonable?\n",
      "\n",
      "Try generating data that has fewer or more clusters. What happens if you tell the clusterer to find fewer clusters than are actually present?\n",
      "\n",
      "The makeClusters has an optional 3rd parameter that sets the spread of the points within each cluster. It is by default set to 20. Try making clusters that overlap by setting the spread higher. For example, the following command should make a pair of clusters that overlap somewhat,\n",
      "     \n",
      "     x = makeClusters(2, 40, 100)\n",
      "\n",
      "Run the clusterer on the overlapping clusters. How does it perform?\n",
      "\n",
      "##4. Experimenting with different distance metrics##\n",
      "\n",
      "The cluster program has the following parameters\n",
      "\n",
      "    cluster(data, N, algorithm_string, display_string)\n",
      "\n",
      "* data \u2013 the feature vectors to be clustered stored as rows of a matrix. \n",
      "* N \u2013 the desired number of clusters to find.\n",
      "* distance_string \u2013 a string describing the distance measure.\n",
      "* display_string \u2013 a string describing the type of animation display.\n",
      "\n",
      "In Section 3 we ran the cluster program with distance_string set to 'lambda x,y:dm_cc_min(x,y,dm_pp_euclid)'. With this setting the clusterer will use a minimum distance cluster-to-cluster measure where point-to-point dissimilarities are computed using a Euclidean distance metric. Other strings that you can use are shown below. (With help from the lecture notes you should be able to guess what they mean).\n",
      "     \n",
      "     'lambda x, y: dm_cc_min(x, y, dm_pp_euclid)'\n",
      "     'lambda x, y: dm_cc_max(x, y, dm_pp_euclid)'\n",
      "     'lambda x, y: dm_cc_mean(x, y, dm_pp_euclid)'\n",
      "     'lambda x, y: dm_cc_average(x, y, dm_pp_euclid)'\n",
      "     \n",
      "     'lambda x, y: dm_cc_min(x, y, dm_pp_manhattan)'\n",
      "     'lambda x, y: dm_cc_max(x, y, dm_pp_manhattan)'\n",
      "     'lambda x, y: dm_cc_mean(x, y, dm_pp_manhattan)'\n",
      "     'lambda x, y: dm_cc_average(x, y, dm_pp_manhattan)'\n",
      "     \n",
      "     'lambda x, y: dm_cc_min(x, y, dm_pp_negative_cosine)'\n",
      "     'lambda x, y: dm_cc_max(x, y, dm_pp_negative_cosine)'\n",
      "     'lambda x, y: dm_cc_mean(x, y, dm_pp_negative_cosine)'\n",
      "     'lambda x, y: dm_cc_average(x, y, dm_pp_negative_cosine)'\n",
      "     \n",
      "(Note \u2018cosine distance\u2019 is a similarity measure. This particular clustering code expects the proximity to be measured in terms of dissimilarity. So it is actually negative cosine distance that is used).\n",
      "\n",
      "Try reclustering the same 2D data using different distance measures."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import clear_output\n",
      "from __future__ import print_function\n",
      "import itertools\n",
      "\n",
      "def cluster(points, numclusters, distance_string, display_string):  \n",
      "\n",
      "    # set the cluster distance measure\n",
      "    dm_cc = eval(distance_string) \n",
      "    \n",
      "    # set display function\n",
      "    display_function = eval(display_string)\n",
      "    \n",
      "    # Initial R0 clustering represented as a set of sets\n",
      "    c = set()\n",
      "    for i in xrange(points.shape[0]):\n",
      "        s = set([i])\n",
      "        c.add(frozenset(s))\n",
      "    \n",
      "    display_function(points, c)\n",
      "\n",
      "    while len(c) > numclusters:\n",
      "        c1, c2 = find_closest_pair(points, c, dm_cc)\n",
      "        merge_clusters(c, c1, c2)\n",
      "        clear_output(wait=True)\n",
      "        display_function(points, c)\n",
      "        print(\"Nclusters remaining = %d\" % len(c), end='\\r')\n",
      "\n",
      "    print()\n",
      "    \n",
      "    display_function(points,c)\n",
      "\n",
      "    return c\n",
      "\n",
      "\n",
      "def find_closest_pair(points, c, dm_cc):\n",
      "\n",
      "    best_c1 = best_c2 = None\n",
      "    mindist = float(\"inf\")\n",
      "    pairs = itertools.combinations(c, 2)\n",
      "    for pair in pairs:\n",
      "        if len(pair[0]) > 0 and len(pair[1]) > 0:\n",
      "            dist = dm_cc(points[list(pair[0]), :], points[list(pair[1]), :])\n",
      "            if dist < mindist:\n",
      "                mindist = dist\n",
      "                best_c1 = pair[0]\n",
      "                best_c2 = pair[1]\n",
      "\n",
      "    return (best_c1, best_c2)\n",
      "\n",
      "\n",
      "# Merge cluster c2 into c1 and remove c2 from set of clusters\n",
      "def merge_clusters(c, c1, c2):\n",
      "    c3 = frozenset.union(c1, c2)\n",
      "    c.remove(c1)\n",
      "    c.remove(c2)\n",
      "    c.add(c3)\n",
      "    \n",
      "# Make scatter plot of points and colour code them according to the supplied clusters\n",
      "\n",
      "def display_points(points, clusters):\n",
      "\n",
      "    i = 0\n",
      "    for cluster in clusters:\n",
      "        cluster_colour = get_cluster_colour(i)\n",
      "        i += 1\n",
      "        plt.scatter(points[list(cluster), 0], points[list(cluster), 1], 50, cluster_colour)\n",
      "    plt.show()\n",
      "            \n",
      "def display_images(points, clusters):\n",
      "    bigimage=np.zeros((480,(len(clusters)+1)*30))\n",
      "    y = 0\n",
      "    for cluster in clusters:\n",
      "        x = 0\n",
      "        for p in cluster:\n",
      "            bigimage[x:(x+30), y:(y+30)] = np.reshape(points[p,:], (30,30), order='F')\n",
      "            x += 30\n",
      "        y += 30\n",
      "        \n",
      "    plt.imshow(bigimage, cmap = cm.Greys)\n",
      "\n",
      "\n",
      "def get_cluster_colour(i):\n",
      "\n",
      "    colour_list = np.array([\n",
      "        [1, 0, 0], [0, 1, 0], [0, 0, 1],\n",
      "        [0, 1, 1], [1, 0, 1], [1, 1, 0] ]);\n",
      "    if (i<6):\n",
      "        cluster_colour = colour_list[i, :]\n",
      "    else:\n",
      "        cluster_colour=np.random.uniform(0, 1, 3)  # vector of 1x3 rand numbers from 0 to 1 JPB\n",
      "    \n",
      "    return cluster_colour\n",
      "\n",
      "\n",
      "# Cluster to cluster distances\n",
      "\n",
      "def dm_cc_max(c1, c2, dm_pp):\n",
      "    distm = dm_cc_inner(c1, c2, dm_pp)\n",
      "    return np.max(distm)\n",
      "\n",
      "def dm_cc_min(c1, c2, dm_pp):\n",
      "    distm = dm_cc_inner(c1, c2, dm_pp)\n",
      "    return np.min(distm)\n",
      "\n",
      "def dm_cc_mean(c1, c2, dm_pp):\n",
      "    distm = dm_cc_inner(c1, c2, dm_pp)\n",
      "    return np.mean(distm);\n",
      "\n",
      "def dm_cc_average(c1, c2, dm_pp):\n",
      "    cr1 = np.mean(c1)\n",
      "    cr2 = np.mean(c2)\n",
      "    return dm_pp(cr1, cr2)\n",
      "\n",
      "def dm_cc_inner(c1, c2, dm_pp):\n",
      "    i = 0\n",
      "    distm = np.empty(len(c1) * len(c2))\n",
      "    for p1 in c1:\n",
      "        for p2 in c2:\n",
      "            distm[i] = dm_pp(p1, p2)\n",
      "            i += 1\n",
      "    return distm\n",
      "\n",
      "# Point to cluster distances\n",
      "\n",
      "def dm_pc_max(p1, c, dm_pp):\n",
      "    distv = dm_pc_inner(p1, c ,dm_pp)\n",
      "    return  np.max(distv)\n",
      "\n",
      "def dm_pc_min(p1, c, dm_pp):\n",
      "    distv = dm_pc_inner(p1, c, dm_pp)\n",
      "    return np.min(distv)\n",
      "\n",
      "def dm_pc_mean(p1, c, dm_pp):\n",
      "    distv = dm_pc_inner(p1, c, dm_pp)\n",
      "    return np.mean(distv)\n",
      "\n",
      "def dm_pcr(p1,c, dm_pp):\n",
      "    cr = np.mean(list(c), axis=1)\n",
      "    return dm_pp_inner(p1, cr)\n",
      "\n",
      "def dm_pc_inner(p1, c, dm_pp):\n",
      "    for p in c:\n",
      "        dist[i] = dm_pp(p1, p)\n",
      "    return dist\n",
      "\n",
      "# Point to point distances\n",
      "\n",
      "def dm_pp_euclid(p1, p2):\n",
      "    diff = p1 - p2\n",
      "    return np.sqrt(np.sum(diff * diff))\n",
      "\n",
      "def dm_pp_manhattan(p1, p2):\n",
      "    return np.sum(np.abs(p1 - p2))\n",
      "\n",
      "def dm_pp_negative_cosine(p1, p2):\n",
      "    return 1.0 - sm_pp_cosine(p1, p2)\n",
      "\n",
      "def sm_pp_inner(p1, p2):\n",
      "    return 1.0/(np.dot(p1, p2.transpose()))\n",
      "\n",
      "def sm_pp_cosine(p1, p2):\n",
      "    return np.dot(p1, p2.transpose()) / (veclen(p1) * veclen(p2))\n",
      "\n",
      "def veclen(x):\n",
      "    return np.sqrt(np.sum(x * x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = makeClusters(5, 10, 400)\n",
      "cluster(x, 5, 'lambda x, y: dm_cc_min(x,y,dm_pp_euclid)', 'display_points')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(test_data.shape)\n",
      "c = cluster(test_data[0:50, :], 26, \"lambda x, y: dm_cc_mean(x, y, dm_pp_negative_cosine)\", 'display_images')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##5. Clustering the letter data.##\n",
      "\n",
      "We are now going to try and cluster the 900 dimensional letter data. We will set the desired number of clusters to be 26 because we know there are 26 different letters in the alphabet. I have provided a special display option, \u2018@display_images\u2019, that will show the letters as images rather than as 2D points.\n",
      "\n",
      "The test data has 200 letters but we will only use the first 100. (If you want to use more you will have to be very patient!) To cluster the first 100 letters type the following command very carefully,\n",
      "\n",
      "    c=cluster(test_data(0:100, :), 26, '@(x,y)dm_cc_mean(x, y, @dm_pp_negative_cosine)', '@display_images')"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c=cluster(test_data[0:50, :], 26, \"lambda x, y: dm_cc_mean(x, y, dm_pp_negative_cosine)\", 'display_images')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once the code starts running you will see that clusters are displayed as separate columns of letter images. Letters will start out in 100 separate columns and by the end of the process there will be just 26 columns remaining. (You may need to stretch the Figure window to be short and wide to make the letter tiles square enough to read clearly).\n",
      "\n",
      "Are the letters clustering nicely?\n",
      "\n",
      "##6. Clustering with Principal Component Analysis features##\n",
      "\n",
      "We will now try clustering the letters in a lower dimensional space. We will use PCA to perform the dimensionality reduction. Let\u2019s reduce the dimensionality from 900 down to just 10.\n",
      "\n",
      "We have seen how to do this in the last lab class, but I have provided a handy function for you called doPCA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def doPCA(data, N):\n",
      "\n",
      "    # compute data covariance matrix\n",
      "    covx = np.cov(data, rowvar=0)\n",
      "    # compute first N pca axes\n",
      "    Norig = covx.shape[0]\n",
      "    [d, v] = scipy.linalg.eigh(covx, eigvals=(Norig-N, Norig-1))\n",
      "    v = np.fliplr(v)\n",
      "    \n",
      "    # compute the mean vector \n",
      "    datamean = np.mean(data)\n",
      "\n",
      "    # subtract mean from all data points\n",
      "    centered = data - datamean\n",
      "    \n",
      "    # project points onto PCA axes\n",
      "    x = np.dot(centered, v)\n",
      "\n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To apply PCA-based dimensionality reduction to the letter data we can now type,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy\n",
      "pca_data = doPCA(test_data, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then type,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(pca_data.shape)\n",
      "c = cluster(pca_data[0:100, :], 26, 'lambda x, y: dm_cc_mean(x, y, dm_pp_euclid)', 'display_points')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember, the first PCA coefficient is often not very discriminative. So we might get a better results doing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = cluster(pca_data[0:100, 1:10], 26, 'lambda x, y: dm_cc_mean(x, y, dm_pp_euclid)', 'display_points');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note, we are using \u2018display_points\u2019 (display_images was only written to work with 900 element image vectors). However, there is a problem when trying to visualize clusters in more than two dimensions. display_points can only show 2D points so it only uses the first two components of each feature vector. However, some points that are far apart in 10-D space can look quite close when seen in 2-D so the clustering might not seem sensible when seen in the 2-D plane. (Consider, by analogy, stars that are grouped into the same constellation that can often be hundreds of light years apart and only appear to be close together when seen on the 2D sky. This 'apparent closeness of distant points\u2019 effect will occur far more often when projecting from a 10-D space onto a 2-D plane).\n",
      "\n",
      "##7. Evaluating the clustering algorithm##\n",
      "\n",
      "We need to find an objective way to evaluate the clusterings. This is possible because we happen to have access to the correct labels. (In many problems the \u2018correct\u2019 label may be unknown or unknowable).\n",
      "\n",
      "The cluster code returns a set of set that stores which points are in each cluster. We have stored this result in the variable c. Set can be converted into simple lists using list()\n",
      "\n",
      "Type list(list(c)[0]) and MATLAB will display the set of indexes of test_data examples that have been put in the first cluster. Type list(list(c)[1]) and you will see the second cluster, etc. \n",
      "\n",
      "To then see the labels of the members of each cluster we can do,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for index in xrange(26):\n",
      "    print(test_labels[0, list(list(c)[index])])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the clustering were perfect all the labels in one particular cluster would be the same.\n",
      "\n",
      "Think how you could write a bit of code that would take the cell array, c, and the label data, test_labels, and return a score telling you how many letters are in \u2018the wrong\u2019 cluster. This seems straightforward at first sight, but after some thinking you\u2019ll see it is a difficult problem in itself. (I\u2019ll provide some code after you\u2019ve had a chance to think about it. You will need this evaluation code for testing the performance of the features you submit for the challenge described below.)\n",
      "\n",
      "\n",
      "##CHALLENGE: Deadline Sunday 1st December##\n",
      "\n",
      "I will be awarding another prize to the person who can find a 10 dimensional representation of the letter data and a distance metric that together provide the best agglomerative clustering.\n",
      "\n",
      "To enter the challenge you need to submit code that will take a matrix of 100x900 pixels representing 100 letters and will return a matrix of 100x10 values representing the dimensionally reduced data. You must also state the distance_string that you want the cluster program to use.\n",
      "\n",
      "To avoid excessive \u2018overfitting\u2019 I will be evaluating the features on a secret random set of 100 examples from the training data. But feel free to test your code on as many different 100 sample subsets as your wish in order to tune your solution.\n",
      "\n",
      "There will be a prize for the winning entry awarded during the Friday 5th December lecture."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#SOLUTION"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    }
   ],
   "metadata": {}
  }
 ]
}