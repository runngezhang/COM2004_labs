{
 "metadata": {
  "name": "",
  "signature": "sha256:fa4011df5ba5037344c9890d8a7f0f5bda258103a07182c6c27094e0f5617256"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "#COM2004/3004 Lab Week 10#\n",
      "\n",
      "#*k*-means clustering#\n",
      "\n",
      "##Objective##\n",
      "\n",
      "In the lab you will use hard k-means clustering to cluster a set of 2D points. You will investigate how the results depends on the initial cluster centre estimates.\n",
      "\n",
      "**This notebook should not take long to complete. Please use the remaining time to make progress on your assignment.**\n",
      "\n",
      "##Background##\n",
      "\n",
      "In an earlier lab class you experimented with agglomerative clustering. In this class you will be using a function optimization approach called hard k-means clustering. An efficient implementation has been provided for you.\n",
      "\n",
      "##1. Overview of key functions##\n",
      "\n",
      "The lab class will make use of the following functions that will be introduced in the notebook as they are needed.\n",
      "\n",
      "* kmeans \u2013 code to perform the clustering.\n",
      "* makeClusters \u2013 same as last week: makes clustered data in which clusters have different shapes \n",
      "* makeClustersEqual \u2013 as above except the clusters all have the same shape (i.e. equal covariance)\n",
      "\n",
      "##2. Generate and display some data to be clustered##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def makeClustersEqual(nclusters, clustersize, spread=20):\n",
      "    \"\"\" \n",
      "    makeClustersEqual - generated 2-D samples lying in\n",
      "        equally-shaped spherical clusters. Cluster centres\n",
      "        are randomly positioned.\n",
      "    \n",
      "    paramaters:\n",
      "      nclusters - number of clusters to generate\n",
      "      clustersize - number of points in each cluster\n",
      "      spread - cluster spread (i.e. variance)\n",
      "      \n",
      "    returns: \n",
      "      x - data matrix storing 2-d samples as rows\n",
      "    \"\"\"\n",
      "    \n",
      "    x = None\n",
      "    pcov = np.diag((spread, spread))\n",
      "    for i in xrange(nclusters):\n",
      "        pmean = np.random.standard_normal((2,)) * 100  # random centres\n",
      "        y = np.random.multivariate_normal(pmean, pcov, clustersize)\n",
      "        x = y if x is None else np.vstack((x, y))\n",
      "    return x"
     ],
     "language": "python",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Use the function makeClustersEqual to make some data, x, that falls into 2 clusters each containing 100 points.\n",
      "          \n",
      "          npoints = 100\n",
      "          nclusters = 2\n",
      "          x = makeClustersEqual(nclusters, npoints)       "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "npoints = 100\n",
      "nclusters = 2\n",
      "x = makeClustersEqual(nclusters, npoints)  "
     ],
     "language": "python",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Display the data using a scatter plot,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "plt.scatter(x[:, 0], x[:, 1])"
     ],
     "language": "python",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "##3. Running the *k*-means algorithm##\n",
      "\n",
      "Below is th function kmeans. Study it carefully. Check it against your lecture notes and **make sure that you understand how it is working**. It has been written to be efficient so you might have to look at it quite carefully to really understand what each line is doing. Ask a demonstrator for help if it is not clear. There are a few comments in the code that should help."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def kmeans (X, w, do_plot=False):\n",
      "    \"\"\" kmean - perform hard kmeans clustering\n",
      "    \n",
      "    parameters:\n",
      "      X - the set of N l-dimensional data points to be clustered\n",
      "      w - the initial estimate of the m cluster centres\n",
      "      do_plot - if true then an animated scatter plot is produced\n",
      "    \n",
      "    returns: w, bel \n",
      "      w, matrix storing the cluster centres as rows \n",
      "      bel, a vector containg the cluster index for each point\n",
      "    \"\"\"\n",
      "    \n",
      "    [N, l] = X.shape\n",
      "    [m, l] = w.shape\n",
      "    e = 1   # measures cluster centre movement\n",
      "    iter = 0  \n",
      "    # iterate until the cluster centres stop moving\n",
      "    while e > 0: \n",
      "        print iter\n",
      "        iter += 1\n",
      "        w_old = w.copy()\n",
      "        dist_all = None\n",
      "        # compute distances from points to cluster centres\n",
      "        for j in xrange(m):\n",
      "            dist = np.sum((np.square(np.dot(np.ones((N, 1)), w[j:j+1, :]) - X)).transpose(), axis=0)\n",
      "            dist_all = dist if dist_all is None else np.vstack((dist_all, dist))\n",
      "\n",
      "        # find closest cluster for each point\n",
      "        bel = np.argmin(dist_all, axis=0)[np.newaxis];\n",
      "        \n",
      "        # compute new cluster centres (ignore clusters with no points)\n",
      "        plt.cla()\n",
      "        for j in xrange(m):\n",
      "            if np.sum(bel==j) is not 0: \n",
      "                w[j, :] = np.sum(X * np.dot((bel==j).transpose(), np.ones((1, l))), axis=0) / np.sum(bel==j)\n",
      "            if do_plot: \n",
      "                plotCluster(X[(bel==j)[0, :], :], w[j, :], j)\n",
      "        if do_plot:\n",
      "            plt.show()\n",
      "        # compute the distance the cluster centres have moved\n",
      "        e = np.sum(np.sum(np.abs(w - w_old)))\n",
      "        \n",
      "    return w, bel[0, :]  \n",
      "\n",
      "def plotCluster(X, center, class_index):\n",
      "    \"\"\" \n",
      "    plotCluster - display clustered data using scatter plot\n",
      "\n",
      "    paramaters:\n",
      "      X - data matrix, N 2-D samples stored as rows\n",
      "      center - 2-D cluster centers stored as matrix rows\n",
      "      class_index - vector of N integer class labels \n",
      "    \"\"\"\n",
      "    colour_list = np.array([\n",
      "        [1, 0, 0], [0, 1, 0], [0, 0, 1],\n",
      "        [0, 1, 1], [1, 0, 1], [1, 1, 0] ])\n",
      "    ncolours = colour_list.shape[0]\n",
      "    class_index %= ncolours\n",
      "    plt.scatter(X[:, 0], X[:, 1], s=30, c=colour_list[class_index, :])\n",
      "    plt.scatter(center[0], center[1], s=80, c=0)\n"
     ],
     "language": "python",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "The *k*-means algorithm can be initialized by randomly assigning positions to the cluster centres. We are going to set the x and y coordinates of each cluster centre to be random numbers between 0 and 100.\n",
      "          \n",
      "          ncluster = 2\n",
      "          w = np.random.uniform(0, 1,  (nclusters, 2)) * 100\n",
      "\n",
      "Now we are ready to perform the clustering. Run the clusterer by typing,\n",
      "          \n",
      "          [w2, bel] = kmeans(x, w, True)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure\n",
      "nclusters = 2\n",
      "x = makeClustersEqual(nclusters, 60, 3000)  \n",
      "w = np.random.uniform(0, 1,  (nclusters, 2)) - 0.5 * 200\n",
      "[w2, bel] = kmeans(x, w, True)"
     ],
     "language": "python",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "The last parameter, \u2018True\u2019, is a switch that tells kmeans to produce scatter plots for each iteration of the clustering. If you don\u2019t want to see the plots then either set this to \u2018False\u2019 or simply omit the parameter (the code runs faster when plotting is turned off).\n",
      "\n",
      "Look at w2. This will show you the final estimate of the cluster centres stored as a pair of column vectors. The index of the cluster to which each point belongs is stored in the vector bel.\n",
      "\n",
      "If you have run kmeans with the plotting option turned off you will need to display the output clusters by using a scatter plot and distinguishing clusters by using either red or blue points. We can select all the points belonging to cluster 1 by using x[bel==0, :] and cluster 2 with x[bel==1, :]. So we can colour code the scatter plot by typing,\n",
      "          \n",
      "          plt.scatter(x[bel==0, 0], x[bel==0, 1], c='r')\n",
      "          plt.scatter(x[bel==1, 0], x[bel==1, 1], c='b')\n",
      "   \n",
      "Try this in the cell below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#SOLUTION\n",
      "plt.scatter(x[bel==0, 0], x[bel==0, 1], c='r')\n",
      "plt.scatter(x[bel==1, 0], x[bel==1, 1], c='b')         "
     ],
     "language": "python",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "##4. Experimenting with different initializations##\n",
      "\n",
      "Now, in the cell below, rerun cluster with the same data but a different random position for the initial cluster centre. Compare the final clustering to the one above. Are they always the same? Run the code several times over and observe what happens. You should see that sometime the clustering works well and other times it produces unexpected results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#SOLUTION"
     ],
     "language": "python",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "##5. Experimenting with harder problems##\n",
      "\n",
      "Now generate data with a larger number of clusters, say four. Run the clustering code multiple times and keep a note of how often it produces a result that looks correct.\n",
      "\n",
      "Try generating data using the makeClusters function from last week (see below) rather than makeClustersEqual. This code can produce clusters that have very different spreads. Does the *k*-means clusterer perform well on this data?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def makeClusters(nclusters, clustersize, spread=None):\n",
      "    \"\"\"\n",
      "    makeClusters - generated 2-D samples lying in\n",
      "                    spherical clusters of different widths\n",
      "    \n",
      "    paramaters:\n",
      "      nclusters - number of clusters to generate\n",
      "      clustersize - number of points in each cluster\n",
      "      spread - cluster spread (i.e. variance)\n",
      "      \n",
      "    returns: \n",
      "      x - data matrix storing 2-d samples as rows\n",
      "    \"\"\"\n",
      "    if spread is None:\n",
      "        spread = 20.0\n",
      "\n",
      "    x = None\n",
      "    for i in xrange(nclusters):\n",
      "        pmean = np.random.standard_normal((2, )) * 100\n",
      "        pcov = np.random.standard_normal((2, 2)) * spread\n",
      "        pcov[0, 1] = pcov[1, 0] = 0\n",
      "        y = np.random.multivariate_normal(pmean, pcov, clustersize)\n",
      "        x = y if x is None else np.vstack((x, y))\n",
      "    return x"
     ],
     "language": "python",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SOLUTION"
     ],
     "language": "python",
     "metadata": {
      "internals": {},
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "You should find that although k-means is very efficient, it only works well in certain situations, and it is quite dependent on the initial cluster position estimates. Soft k-means can give better results. Better still is to use a Gaussian mixture model and estimate the parameters using the EM algorithm. However, none of these optimization techniques is immune to the problem of poor initialization. If the initial cluster centres are too far from the correct values the algorithms are prone to getting stuck in local optima. These local optima may be very different from the overall best clustering."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}