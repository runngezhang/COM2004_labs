{
 "metadata": {
  "name": "",
  "signature": "sha256:dc2b01076163a17a89667444a1fd20e241164c6ef1f35906146b7cafb12f5c6a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#COM2004/3004 Lab Week 11#\n",
      "\n",
      "#Face Recognition#\n",
      "\n",
      "##Objective## \n",
      "\n",
      "In this lab you will use PCA and a Nearest Neighbour classifier to tackle a challenging classification task \u2013 face recognition.\n",
      "\n",
      "#Background#\n",
      "\n",
      "In an earlier lab class you used PCA and a nearest neighbour classifier to tackle a very simple character recognition task. In this lab you will be using the same techniques but applying them to a more challenging task \u2212 face recognition. You are provided with a small data set containing 10 images each of 40 different people, i.e. 400 images in total. You will be splitting this into training data and test data. The classification system will be built using only the training data and then tested on the unseen test data. A jackknife training/testing schedule will be used to reliably test the performance of the system.\n",
      "\n",
      "##1. Overview of key functions##\n",
      "\n",
      "The following functions will be used in this lab class.\n",
      "\n",
      "* faces.tif - the face image data. 400 images stored as one big image. \n",
      "* loadfaces.m \u2013 a function for loading in the face data.\n",
      "* learnPCA.m \u2013 a function for performing PCA analysis.\n",
      "* reducePCA.m \u2013 a function to perform the PCA dimensionality reduction. \n",
      "* computePCAaxes.m \u2013 a helper function for learnPCA.m. \n",
      "* classify.m \u2013 the nearest neighbor classifier that we\u2019ve used before.\n",
      "\n",
      "##2. Loading the face data##\n",
      "\n",
      "The face data is stored as one big image, faces.tif. If you try opening this in Windows you should be able to view it. I have provided a program that segments this image and extracts each of the 400 faces contained in it as a separate feature vector. The individual face images have dimension 46 rows by 56 columns giving the raw feature vectors a dimensionality of 2,576."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.misc\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.cm as cm\n",
      "%matplotlib inline\n",
      "\n",
      "def loadfaces():\n",
      "# Loads faces from the faces.tif face database image\n",
      "# Faces are returned as vectors stored in the rows of a matrix. \n",
      "    x=scipy.misc.imread(\"data/faces.tif\").astype(np.float32)\n",
      "  \n",
      "    # Width and height of an individual face\n",
      "    width, height = 46, 56\n",
      "    nrows, ncols = 20, 20\n",
      "    \n",
      "    # Cut individual faces from the grid of faces\n",
      "    pixels = width * height\n",
      "    faces = np.zeros((nrows*ncols, pixels))\n",
      "    z=0\n",
      "    for a in xrange(nrows):\n",
      "        for b in xrange(ncols):\n",
      "            # Note that faces on the left of the grid are separated from\n",
      "            # those on the right by a column of 2 pixels.\n",
      "            face=x[a*(height+1):a*(height+1)+height, b*(width+1)+(b>9):b*(width+1)+width+(b>9)];\n",
      "            # Reshape the face matrices into vectors and \n",
      "            # store the 400 faces as rows of a large matrix\n",
      "            faces[z,:] = np.reshape(face, (1, pixels));\n",
      "            z += 1\n",
      "            \n",
      "        # Comment these lines out to speed up the loading).\n",
      "        plt.subplot(5,4,a)\n",
      "        plt.imshow(face, cmap=cm.Greys_r);\n",
      "    return faces"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Type,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    data = loadfaces();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This will construct a 400 by 2,576 matrix. To view a single face as an image use,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.imshow(np.reshape(data[0,:], (56, 46)), cmap=cm.Greys_r);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You should find that rows 1 to 10 are images of person 1; rows 11 to 20 are person 2; rows 21 to 30 are person 3 and so on up to person 40.\n",
      "\n",
      "The code below uses the IPython widget library to produce a figure with an interactive control,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.html.widgets import interactive\n",
      "from IPython.display import display\n",
      "print(data.shape)\n",
      "np.reshape(data[0,:], (56, 46), order='F')\n",
      "def show_face(pid=0):\n",
      "    plt.imshow(np.reshape(data[pid*20,:], (56, 46)), cmap=cm.Greys_r)\n",
      "    plt.show()\n",
      "v = interactive(show_face, pid=(0,19))\n",
      "display(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3. Dividing the data into training and testing##\n",
      "\n",
      "We are going to use 9 examples of each person to train the system and 1 example of each person to test the system. We will start by making training and testing labels. We will use an integer coding, i.e. 1 for person 1; 2 for person 2; 3 for person 3 and so on.\n",
      "\n",
      "Making the test labels is easy as there is going to be just one example of each person so,\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_labels = xrange(40)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The training labels need to be a vector containing nine 1s followed by nine 2s and then nine 3s and so on. We could type it directly, but the following hacky bit of MATLAB will do the job,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_labels = np.reshape(np.tile(xrange(40),(9,1)),(1,360), order='F');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need to split up the data itself. We will choose the test data to be the Kth example of each person (where K is from 1 to 10) and the training data will be all the rest. So for example if K=1 the test data would be rows 1, 11, 21, ....391 of data and training data will be the remaining 360 rows.\n",
      "\n",
      "How can we do this neatly? The following achieves the desired result by using a modulus operation,\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "K = 1;\n",
      "train_data = data[(np.mod(xrange(400), 10) + 1) != K, :]\n",
      "test_data = data[(np.mod(xrange(400), 10) + 1) == K, :]\n",
      "print(train_data.shape, test_data.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##4. Performing the dimensionality reduction##\n",
      "\n",
      "We are now going to perform the PCA analysis and use the analysis to reduce the dimensionality of the data. I\u2019ve split the PCA dimensionality reduction into two stages: learnPCA will analyse the training data and find the first N principle component axes. reducePCA will then be used to reduce the dimensionality of the training data and test data by projecting onto these N axes. It is logical to split it this way because the analysis step is only applied to the training data, whereas the dimensionality reduction will be applied to all the data. learnPCA and reducePCA are provided in the two cells below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learnPCA(data, N):\n",
      "# Performs PCA dimensionality reduction of 'data' reducing down to\n",
      "# N dimensions.\n",
      "# Returns:\n",
      "# x - dimensionally reduced data\n",
      "# v - the PCA axes (which may be useful)\n",
      "\n",
      "    ndata = data.shape[0]\n",
      "\n",
      "    # Calculate and display the mean face\n",
      "    mean_vector = np.mean(data, axis=0)\n",
      "\n",
      "    # Calculate the mean normalised data\n",
      "    deltadata = data - mean_vector\n",
      "\n",
      "    # (You'd expect the next line to be deltadata'*deltadata but\n",
      "    # a computational shortcut is being employed...)\n",
      "    u = np.dot(deltadata, deltadata.transpose())\n",
      "    d, pc = scipy.linalg.eigh(u, eigvals=(ndata-N,ndata-1))\n",
      "    pc = np.fliplr(pc)\n",
      "     \n",
      "    pca_axes = np.dot(pc.transpose(), deltadata)\n",
      "\n",
      "    # compute the mean vector \n",
      "    mean_vector = np.mean(data, axis=0)\n",
      "\n",
      "    return pca_axes, mean_vector\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reducePCA(data, pca_axes, mean_vector):\n",
      "# Performs PCA dimensionality reduction\n",
      "\n",
      "    # subtract mean from all data points\n",
      "    centered = data - mean_vector\n",
      "\n",
      "    # project points onto PCA axes\n",
      "    reduced_data = np.dot(centered, pca_axes.transpose())\n",
      "\n",
      "    return reduced_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First perform the analysis step,\n",
      "\n",
      "    nAxes = 10;\n",
      "    [pca_axes, mean_vector] = learnPCA(train_data, nAxes);"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nAxes = 10\n",
      "[pca_axes, mean_vector] = learnPCA(train_data, nAxes);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note the program also returns the mean_vector \u2013 we will need this when performing the projection.\n",
      "\n",
      "To display one of the PCA axes as an image (i.e. an eigenface) you can do,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PCA_N = 0;\n",
      "plt.imshow(np.reshape(pca_axes[PCA_N, :], (56, 46)), cmap = cm.Greys_r);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Look at the first two or three eigenfaces and compare them to the pictures in the lecture notes. \n",
      "\n",
      "The next step is to actually project all the data on to the N PCA axes, i.e. the dimensionality\n",
      "reduction. You can use the program reducePCA to do this,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reduced_train = reducePCA(train_data, pca_axes, mean_vector)\n",
      "reduced_test = reducePCA(test_data, pca_axes, mean_vector)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is worth looking at the code for learnPCA and reducePCA to make sure that it makes sense to\n",
      "you.\n",
      "\n",
      "##5. Performing the classification##\n",
      "\n",
      "We can now perform the classification in the PCA space using our nearest neighbor classifier code. The classify function used in previous lab classes is repeated below"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify(train, train_labels, test, test_labels, features=None):\n",
      "\n",
      "# Nearest neighbour classification.\n",
      "\n",
      "    # Use all feature is no feature parameter has been supplied\n",
      "    if features is None:\n",
      "        features = xrange(train.shape[1])\n",
      "\n",
      "    # Select the desired features from the training and test data\n",
      "    train = train[:, features]\n",
      "    test = test[:, features]\n",
      "    \n",
      "    # Super compact implementation of nearest neighbour \n",
      "    x = np.dot(test, train.transpose())\n",
      "    modtest = np.sqrt(np.sum(test*test, axis=1))\n",
      "    modtrain = np.sqrt(np.sum(train*train, axis=1))\n",
      "    dist = x/np.outer(modtest, modtrain.transpose()); # cosine distance\n",
      "    nearest = np.argmax(dist, axis=1)\n",
      "    mdist = np.max(dist, axis=1)\n",
      "    label = train_labels[0,nearest]\n",
      "    score = (100.0 * sum(test_labels==label))/label.shape[0]\n",
      "\n",
      "    # Construct a confusion matrix\n",
      "#    nclasses = np.max(np.hstack((test_labels,train_labels)))  JPB FIX\n",
      "    nclasses = np.max(train_labels)\n",
      "    confusions=np.zeros((nclasses,nclasses))\n",
      "    for i in xrange(len(test_labels)):\n",
      "        confusions[test_labels[i]-1,label[i]-1]+=1;\n",
      "\n",
      "    return score, confusions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To perform the classification we use the classify function as follows,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score, confusions = classify(reduced_train, train_labels, reduced_test, test_labels)\n",
      "print(score)\n",
      "plt.imshow(confusions);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If all has gone well, the score should be well above 90%. Look also at the confusion matrix.\n",
      "\n",
      "This is fine but we have only classified 40 images so the result is not very statistically significant and the confusion matrix is not very interesting because we haven\u2019t seen enough examples to get a measure of the common confusions.\n",
      "\n",
      "The last step will be to repeat everything we have done so far 10 times where each time we test a different 10th of the data. Looking back at section 3, we can see that by setting K=2 we can re- divide the 400 images so that every 2nd example of each person is now the test image and the remainder is the training data. If we placed a loop of K from 1 to 10 around all the instruction in section 3, 4 and 5 we could perform the testing 10 times each time using a different 10th of the data for testing. We would then simply average the 10 scores coming out of the classify command and sum up the 10 confusion matrices. This is called jackknife testing.\n",
      "\n",
      "*Write the loop and compute the overall score and confusion matrix.*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Look at the confusion matrix. There are some interesting repeated confusions. Is the classifier making mistakes that you can imagine that you might make yourself? Now repeat the classification but this time using a smaller number of PCA coefficients, or a larger number. How does classification performance vary?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    }
   ],
   "metadata": {}
  }
 ]
}